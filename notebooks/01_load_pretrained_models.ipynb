{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pretrained Audio and Music Models\n",
    "\n",
    "This notebook demonstrates how to load and test pretrained models for music generation and understanding.\n",
    "\n",
    "## Models Covered\n",
    "1. MusicGen (Meta AudioCraft) - Text-to-music generation\n",
    "2. MusicVAE (Magenta) - Symbolic music VAE\n",
    "3. DDSP (Magenta) - Differentiable DSP synthesis\n",
    "4. CLAP - Audio-text embeddings\n",
    "\n",
    "## Setup\n",
    "```bash\n",
    "pip install audiocraft magenta transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MusicGen - Text-to-Music Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MusicGen model\n",
    "try:\n",
    "    from audiocraft.models import MusicGen\n",
    "    \n",
    "    print(\"Loading MusicGen model (this may take a few minutes)...\")\n",
    "    model_musicgen = MusicGen.get_pretrained('small')  # Start with small model\n",
    "    model_musicgen.set_generation_params(duration=5)  # 5 second generations\n",
    "    \n",
    "    print(\"✓ MusicGen loaded successfully\")\n",
    "    print(f\"Model device: {model_musicgen.device}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠ AudioCraft not installed. Run: pip install audiocraft\")\n",
    "    model_musicgen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate music from text descriptions\n",
    "if model_musicgen is not None:\n",
    "    descriptions = [\n",
    "        \"calm ambient music with gentle piano\",\n",
    "        \"energetic electronic dance music\",\n",
    "        \"melancholic acoustic guitar\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Generating music samples...\")\n",
    "    wav = model_musicgen.generate(descriptions)\n",
    "    \n",
    "    # Display audio\n",
    "    for idx, desc in enumerate(descriptions):\n",
    "        print(f\"\\n{idx+1}. {desc}\")\n",
    "        audio_np = wav[idx].cpu().numpy().squeeze()\n",
    "        display(Audio(audio_np, rate=model_musicgen.sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MusicVAE - Symbolic Music with Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MusicVAE for symbolic music\n",
    "try:\n",
    "    import magenta\n",
    "    from magenta.models.music_vae import configs\n",
    "    from magenta.models.music_vae.trained_model import TrainedModel\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    print(\"Loading MusicVAE model...\")\n",
    "    # Note: Requires downloading checkpoint files\n",
    "    # See: https://github.com/magenta/magenta/tree/main/magenta/models/music_vae\n",
    "    \n",
    "    print(\"✓ Magenta available\")\n",
    "    print(\"Note: Download checkpoints from Magenta repository for full functionality\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠ Magenta not installed. Run: pip install magenta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CLAP - Audio-Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLAP for audio-text alignment\n",
    "try:\n",
    "    from transformers import ClapModel, ClapProcessor\n",
    "    \n",
    "    print(\"Loading CLAP model...\")\n",
    "    model_clap = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "    processor_clap = ClapProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "    \n",
    "    print(\"✓ CLAP loaded successfully\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠ Transformers not installed. Run: pip install transformers\")\n",
    "    model_clap = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compute text embeddings for music descriptions\n",
    "if model_clap is not None:\n",
    "    text_descriptions = [\n",
    "        \"peaceful meditation music\",\n",
    "        \"energetic rock guitar\",\n",
    "        \"sad piano melody\",\n",
    "        \"upbeat electronic dance\"\n",
    "    ]\n",
    "    \n",
    "    inputs = processor_clap(text=text_descriptions, return_tensors=\"pt\", padding=True)\n",
    "    text_embeds = model_clap.get_text_features(**inputs)\n",
    "    \n",
    "    print(f\"Text embeddings shape: {text_embeds.shape}\")\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity = torch.cosine_similarity(text_embeds.unsqueeze(1), text_embeds.unsqueeze(0), dim=2)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(similarity.detach().numpy(), cmap='viridis', vmin=0, vmax=1)\n",
    "    plt.colorbar(label='Cosine Similarity')\n",
    "    plt.xticks(range(len(text_descriptions)), text_descriptions, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(text_descriptions)), text_descriptions)\n",
    "    plt.title('Music Description Similarity Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DDSP - Differentiable Audio Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDSP demo (requires additional setup)\n",
    "print(\"DDSP: Differentiable Digital Signal Processing\")\n",
    "print(\"For DDSP examples, see: https://github.com/magenta/ddsp\")\n",
    "print(\"\")\n",
    "print(\"DDSP provides interpretable audio synthesis controls:\")\n",
    "print(\"  - Fundamental frequency (pitch)\")\n",
    "print(\"  - Loudness (amplitude)\")\n",
    "print(\"  - Harmonic distribution (timbre)\")\n",
    "print(\"  - Noise filtering\")\n",
    "print(\"\")\n",
    "print(\"These parameters are ideal for brain-conditioned synthesis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### Models Loaded\n",
    "This notebook demonstrated loading:\n",
    "- ✓ MusicGen for text-to-music generation\n",
    "- ✓ CLAP for audio-text embeddings\n",
    "- (Optional) MusicVAE for symbolic music\n",
    "- (Optional) DDSP for controllable synthesis\n",
    "\n",
    "### Key Takeaways\n",
    "1. **MusicGen**: Good for high-level music generation from descriptions\n",
    "2. **CLAP**: Bridges text and audio - useful for semantic conditioning\n",
    "3. **MusicVAE**: Structured latent space for interpolation\n",
    "4. **DDSP**: Interpretable parameters for fine-grained control\n",
    "\n",
    "### Next Steps\n",
    "1. Explore latent spaces of these models (see `02_explore_latent_spaces.ipynb`)\n",
    "2. Simulate neural signals (see `03-05_simulated_*_features.ipynb`)\n",
    "3. Map neural features to model parameters (see `06_latent_space_mapping.ipynb`)\n",
    "\n",
    "### For Real Applications\n",
    "- Consider model size vs. latency trade-offs\n",
    "- Profile memory usage and inference time\n",
    "- Test on target hardware (CPU, GPU, edge devices)\n",
    "- Fine-tune on specific music styles if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
