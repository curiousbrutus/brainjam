{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated EEG Feature Extraction\n",
    "\n",
    "This notebook demonstrates simulating and extracting features from EEG (Electroencephalography) data for real-time brain-music applications.\n",
    "\n",
    "## Background\n",
    "- **EEG** measures electrical activity via scalp electrodes\n",
    "- **Temporal resolution**: ~1ms (excellent for real-time)\n",
    "- **Spatial resolution**: Limited (volume conduction)\n",
    "- **Coverage**: Primarily cortical surface\n",
    "\n",
    "## Use Cases\n",
    "- Real-time brain-music interfaces\n",
    "- Detecting cognitive and emotional states\n",
    "- Tracking temporal dynamics of creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulate EEG Signals\n",
    "\n",
    "Simulate multi-channel EEG during music listening with creative engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "fs = 250  # Sampling frequency (Hz) - typical for EEG\n",
    "duration = 60  # 1 minute recording\n",
    "n_samples = fs * duration\n",
    "time = np.arange(n_samples) / fs\n",
    "\n",
    "# Electrode locations (simplified)\n",
    "channels = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2']\n",
    "n_channels = len(channels)\n",
    "\n",
    "print(f\"Simulating EEG: {n_channels} channels, {duration}s, {fs}Hz\")\n",
    "print(f\"Total samples: {n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eeg_band(freq_range, amplitude, time, fs):\n",
    "    \"\"\"Generate oscillatory activity in specific frequency band\"\"\"\n",
    "    freq = np.random.uniform(freq_range[0], freq_range[1])\n",
    "    phase = np.random.uniform(0, 2*np.pi)\n",
    "    return amplitude * np.sin(2 * np.pi * freq * time + phase)\n",
    "\n",
    "# EEG frequency bands\n",
    "bands = {\n",
    "    'delta': (1, 4),    # Deep sleep\n",
    "    'theta': (4, 8),    # Meditation, creativity\n",
    "    'alpha': (8, 13),   # Relaxed, closed eyes\n",
    "    'beta': (13, 30),   # Active thinking, focus\n",
    "    'gamma': (30, 50)   # High-level cognition\n",
    "}\n",
    "\n",
    "# Simulate EEG for each channel\n",
    "eeg_data = np.zeros((n_channels, n_samples))\n",
    "\n",
    "for ch in range(n_channels):\n",
    "    # Each channel has different mix of frequency bands\n",
    "    # Frontal channels: more theta/beta (executive function)\n",
    "    # Parietal: more alpha (attention)\n",
    "    # Occipital: more alpha (visual processing)\n",
    "    \n",
    "    if 'F' in channels[ch]:  # Frontal\n",
    "        theta = generate_eeg_band(bands['theta'], 15, time, fs)\n",
    "        beta = generate_eeg_band(bands['beta'], 8, time, fs)\n",
    "        alpha = generate_eeg_band(bands['alpha'], 5, time, fs)\n",
    "    elif 'C' in channels[ch]:  # Central\n",
    "        theta = generate_eeg_band(bands['theta'], 10, time, fs)\n",
    "        alpha = generate_eeg_band(bands['alpha'], 12, time, fs)\n",
    "        beta = generate_eeg_band(bands['beta'], 8, time, fs)\n",
    "    elif 'P' in channels[ch]:  # Parietal\n",
    "        alpha = generate_eeg_band(bands['alpha'], 15, time, fs)\n",
    "        theta = generate_eeg_band(bands['theta'], 8, time, fs)\n",
    "        beta = generate_eeg_band(bands['beta'], 5, time, fs)\n",
    "    else:  # Occipital\n",
    "        alpha = generate_eeg_band(bands['alpha'], 18, time, fs)\n",
    "        theta = generate_eeg_band(bands['theta'], 6, time, fs)\n",
    "        beta = generate_eeg_band(bands['beta'], 4, time, fs)\n",
    "    \n",
    "    # Combine bands\n",
    "    eeg_data[ch] = theta + alpha + beta\n",
    "    \n",
    "    # Add pink noise (1/f characteristic of EEG)\n",
    "    pink_noise = np.cumsum(np.random.randn(n_samples)) / np.sqrt(fs)\n",
    "    pink_noise = 5 * (pink_noise - np.mean(pink_noise)) / np.std(pink_noise)\n",
    "    eeg_data[ch] += pink_noise\n",
    "    \n",
    "    # Add occasional artifacts (blinks, muscle)\n",
    "    if 'Fp' in channels[ch]:  # Eye blinks in frontal channels\n",
    "        blink_times = np.random.choice(n_samples, size=20, replace=False)\n",
    "        for t in blink_times:\n",
    "            blink = np.exp(-((np.arange(n_samples) - t)**2) / (0.1*fs)**2)\n",
    "            eeg_data[ch] += 100 * blink\n",
    "\n",
    "print(\"✓ EEG signals simulated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw EEG\n",
    "fig, axes = plt.subplots(n_channels, 1, figsize=(14, 12))\n",
    "plot_duration = 5  # Show first 5 seconds\n",
    "plot_samples = int(plot_duration * fs)\n",
    "\n",
    "for i, (ax, ch_name) in enumerate(zip(axes, channels)):\n",
    "    ax.plot(time[:plot_samples], eeg_data[i, :plot_samples], linewidth=0.5)\n",
    "    ax.set_ylabel(ch_name)\n",
    "    ax.set_ylim(-50, 50)\n",
    "    ax.grid(alpha=0.3)\n",
    "    if i < n_channels - 1:\n",
    "        ax.set_xticks([])\n",
    "\n",
    "axes[-1].set_xlabel('Time (s)')\n",
    "fig.suptitle('Simulated EEG Signals (First 5 seconds)', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Power Spectral Density Analysis\n",
    "\n",
    "Extract frequency band power - key features for EEG analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_band_power(eeg_signal, fs, band_range):\n",
    "    \"\"\"Compute power in specific frequency band\"\"\"\n",
    "    # Compute power spectral density\n",
    "    freqs, psd = signal.welch(eeg_signal, fs, nperseg=2*fs)\n",
    "    \n",
    "    # Find indices for frequency band\n",
    "    idx_band = np.logical_and(freqs >= band_range[0], freqs <= band_range[1])\n",
    "    \n",
    "    # Compute band power\n",
    "    band_power = np.trapz(psd[idx_band], freqs[idx_band])\n",
    "    return band_power\n",
    "\n",
    "# Compute band powers for all channels\n",
    "band_powers = {band: np.zeros(n_channels) for band in bands.keys()}\n",
    "\n",
    "for ch in range(n_channels):\n",
    "    for band_name, band_range in bands.items():\n",
    "        band_powers[band_name][ch] = compute_band_power(eeg_data[ch], fs, band_range)\n",
    "\n",
    "print(\"✓ Band powers computed for all channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize band powers\n",
    "fig, axes = plt.subplots(1, len(bands), figsize=(16, 4))\n",
    "\n",
    "for ax, (band_name, powers) in zip(axes, band_powers.items()):\n",
    "    ax.bar(channels, powers)\n",
    "    ax.set_title(f'{band_name.capitalize()} ({bands[band_name][0]}-{bands[band_name][1]} Hz)')\n",
    "    ax.set_ylabel('Power (µV²)')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topographic visualization (simplified)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Show theta, alpha, beta as heatmaps\n",
    "for ax, band_name in zip(axes, ['theta', 'alpha', 'beta']):\n",
    "    # Simplified 2D layout\n",
    "    topo_data = band_powers[band_name].reshape(2, 5)  # 2 rows, 5 cols\n",
    "    im = ax.imshow(topo_data, cmap='RdYlBu_r', aspect='auto')\n",
    "    ax.set_title(f'{band_name.capitalize()} Power')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time-Frequency Analysis\n",
    "\n",
    "Track how frequency content changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectrogram for one channel\n",
    "channel_idx = 2  # F3 channel\n",
    "freqs, times_spec, Sxx = signal.spectrogram(eeg_data[channel_idx], fs, \n",
    "                                            nperseg=2*fs, noverlap=fs)\n",
    "\n",
    "# Plot spectrogram\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.pcolormesh(times_spec, freqs, 10 * np.log10(Sxx), \n",
    "               shading='gouraud', cmap='viridis')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.title(f'Spectrogram: {channels[channel_idx]}')\n",
    "plt.ylim(0, 50)\n",
    "plt.colorbar(label='Power (dB)')\n",
    "\n",
    "# Mark frequency bands\n",
    "for band_name, (low, high) in bands.items():\n",
    "    plt.axhline(low, color='white', linestyle='--', alpha=0.3, linewidth=0.5)\n",
    "    plt.axhline(high, color='white', linestyle='--', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Real-Time Features\n",
    "\n",
    "Compute features suitable for real-time brain-music interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_realtime_features(eeg_window, fs, channels):\n",
    "    \"\"\"Extract features from EEG window for real-time use\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # 1. Band power ratios (indicators of cognitive state)\n",
    "    theta_power = np.mean([compute_band_power(eeg_window[ch], fs, bands['theta']) \n",
    "                          for ch in range(len(channels))])\n",
    "    alpha_power = np.mean([compute_band_power(eeg_window[ch], fs, bands['alpha']) \n",
    "                          for ch in range(len(channels))])\n",
    "    beta_power = np.mean([compute_band_power(eeg_window[ch], fs, bands['beta']) \n",
    "                         for ch in range(len(channels))])\n",
    "    \n",
    "    features['theta_alpha_ratio'] = theta_power / (alpha_power + 1e-10)\n",
    "    features['beta_alpha_ratio'] = beta_power / (alpha_power + 1e-10)\n",
    "    \n",
    "    # 2. Frontal alpha asymmetry (emotion/motivation)\n",
    "    left_frontal = compute_band_power(eeg_window[2], fs, bands['alpha'])  # F3\n",
    "    right_frontal = compute_band_power(eeg_window[3], fs, bands['alpha'])  # F4\n",
    "    features['frontal_asymmetry'] = np.log(right_frontal) - np.log(left_frontal + 1e-10)\n",
    "    \n",
    "    # 3. Engagement index (beta/theta+alpha)\n",
    "    features['engagement'] = beta_power / (theta_power + alpha_power + 1e-10)\n",
    "    \n",
    "    # 4. Overall activation (total power)\n",
    "    features['activation'] = np.mean([np.var(eeg_window[ch]) for ch in range(len(channels))])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Compute features over sliding windows\n",
    "window_size = 2 * fs  # 2 second windows\n",
    "hop_size = fs // 2  # 0.5 second hops\n",
    "n_windows = (n_samples - window_size) // hop_size\n",
    "\n",
    "feature_timeline = {\n",
    "    'theta_alpha_ratio': [],\n",
    "    'beta_alpha_ratio': [],\n",
    "    'frontal_asymmetry': [],\n",
    "    'engagement': [],\n",
    "    'activation': []\n",
    "}\n",
    "\n",
    "for i in range(n_windows):\n",
    "    start = i * hop_size\n",
    "    end = start + window_size\n",
    "    window = eeg_data[:, start:end]\n",
    "    \n",
    "    feats = extract_realtime_features(window, fs, channels)\n",
    "    for key in feature_timeline.keys():\n",
    "        feature_timeline[key].append(feats[key])\n",
    "\n",
    "# Convert to arrays\n",
    "for key in feature_timeline.keys():\n",
    "    feature_timeline[key] = np.array(feature_timeline[key])\n",
    "\n",
    "times_features = np.arange(n_windows) * (hop_size / fs) + (window_size / fs / 2)\n",
    "\n",
    "print(f\"✓ Extracted {len(feature_timeline)} features over {n_windows} windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize real-time features\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 10))\n",
    "\n",
    "feature_labels = {\n",
    "    'theta_alpha_ratio': 'Theta/Alpha Ratio\\n(Creativity/Relaxation)',\n",
    "    'beta_alpha_ratio': 'Beta/Alpha Ratio\\n(Attention/Relaxation)',\n",
    "    'frontal_asymmetry': 'Frontal Alpha Asymmetry\\n(Approach/Avoidance)',\n",
    "    'engagement': 'Engagement Index\\n(Beta/Theta+Alpha)',\n",
    "    'activation': 'Overall Activation\\n(Total Power)'\n",
    "}\n",
    "\n",
    "for ax, (key, label) in zip(axes, feature_labels.items()):\n",
    "    ax.plot(times_features, feature_timeline[key], linewidth=2)\n",
    "    ax.set_ylabel(label, fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Time (s)')\n",
    "fig.suptitle('Real-Time EEG Features Over Time', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Did\n",
    "1. Simulated realistic multi-channel EEG with frequency band structure\n",
    "2. Computed power spectral density and band powers\n",
    "3. Performed time-frequency analysis\n",
    "4. Extracted real-time features for brain-music mapping:\n",
    "   - Band power ratios (cognitive states)\n",
    "   - Frontal asymmetry (emotion/motivation)\n",
    "   - Engagement index (attention)\n",
    "   - Overall activation\n",
    "\n",
    "### Key Insights\n",
    "- EEG provides **high temporal resolution** - ideal for real-time\n",
    "- **Frequency bands** correspond to different cognitive states\n",
    "- **Spatial patterns** reveal lateralized processes (emotion, attention)\n",
    "- Features can be computed in **sliding windows** for continuous tracking\n",
    "\n",
    "### Mapping to Music\n",
    "These features can control musical parameters:\n",
    "- **Theta/Alpha ratio** → Harmonic complexity (higher = more complex)\n",
    "- **Engagement** → Tempo/energy (higher = faster/louder)\n",
    "- **Frontal asymmetry** → Valence (positive = major, negative = minor)\n",
    "- **Activation** → Overall intensity/density\n",
    "\n",
    "### Next Steps\n",
    "1. Map features to music latent spaces (see `06_latent_space_mapping.ipynb`)\n",
    "2. Implement real-time processing pipeline (see `toy_interface/`)\n",
    "3. Test with actual EEG hardware\n",
    "\n",
    "### Advantages Over fMRI\n",
    "- ✓ Real-time capability (<100ms latency possible)\n",
    "- ✓ Portable and affordable\n",
    "- ✓ Natural listening environment\n",
    "- ✗ Limited spatial resolution\n",
    "- ✗ Sensitive to artifacts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
