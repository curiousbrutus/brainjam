{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Control to Sound Demo\n",
    "\n",
    "This notebook demonstrates the core BrainJam performance loop:\n",
    "- **Input**: Keyboard or mock EEG controls\n",
    "- **Mapping**: Control parameters to synthesis parameters\n",
    "- **Synthesis**: Real-time audio generation\n",
    "- **Feedback**: Immediate audio output\n",
    "\n",
    "**Target Latency**: < 100ms end-to-end\n",
    "\n",
    "---\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- This is a **performance instrument**, not brain decoding\n",
    "- Brain signals (when used) are **control inputs**, comparable to breath or gesture\n",
    "- You maintain **full creative control** - this is not autonomous AI\n",
    "- Start with mock EEG or keyboard to understand the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display, clear_output\n",
    "import time\n",
    "\n",
    "# Import BrainJam performance system\n",
    "from performance_system.controllers import MockEEGController\n",
    "from performance_system.sound_engines import ParametricSynth\n",
    "from performance_system.mapping_models import LinearMapper\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the Performance System\n",
    "\n",
    "We'll set up:\n",
    "- **Controller**: Mock EEG (generates structured control signals)\n",
    "- **Synthesizer**: Parametric synth with 4 controllable parameters\n",
    "- **Mapper**: Optional mapping layer (identity for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize controller\n",
    "controller = MockEEGController(fs=250)  # 250 Hz sampling rate\n",
    "print(\"✓ Mock EEG Controller initialized\")\n",
    "print(\"  Generates 4 continuous control signals (0-1 range)\")\n",
    "print(\"  NOT decoding thoughts - just structured test signals\")\n",
    "\n",
    "# Initialize synthesizer\n",
    "synth = ParametricSynth(sample_rate=44100, base_freq=220.0)  # A3\n",
    "print(\"\\n✓ Parametric Synthesizer initialized\")\n",
    "print(\"  Controllable parameters:\")\n",
    "print(\"    - tempo_density: Event rate (0=sparse, 1=dense)\")\n",
    "print(\"    - harmonic_tension: Dissonance (0=consonant, 1=tense)\")\n",
    "print(\"    - spectral_brightness: Timbre (0=dark, 1=bright)\")\n",
    "print(\"    - noise_balance: Texture (0=tonal, 1=noisy)\")\n",
    "\n",
    "# Optional: Initialize mapper (identity mapping for now)\n",
    "mapper = LinearMapper(n_inputs=4, n_outputs=4)\n",
    "print(\"\\n✓ Linear Mapper initialized (identity mapping)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test the Control Signal\n",
    "\n",
    "Let's see what kind of control signals the mock EEG generates.\n",
    "These are continuous, slowly-varying parameters - NOT decoded mental states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sequence of control vectors\n",
    "n_samples = 100\n",
    "control_history = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    controls = controller.get_control_vector(duration=0.1)\n",
    "    control_history.append(controls)\n",
    "\n",
    "# Convert to arrays for plotting\n",
    "control_1 = [c['control_1'] for c in control_history]\n",
    "control_2 = [c['control_2'] for c in control_history]\n",
    "control_3 = [c['control_3'] for c in control_history]\n",
    "control_4 = [c['control_4'] for c in control_history]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(4, 1, figsize=(12, 8))\n",
    "time_axis = np.arange(n_samples) * 0.1  # Time in seconds\n",
    "\n",
    "axes[0].plot(time_axis, control_1, 'b-', linewidth=2)\n",
    "axes[0].set_ylabel('Control 1\\n(Tempo/Density)', fontsize=10)\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(time_axis, control_2, 'g-', linewidth=2)\n",
    "axes[1].set_ylabel('Control 2\\n(Harmony)', fontsize=10)\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(time_axis, control_3, 'r-', linewidth=2)\n",
    "axes[2].set_ylabel('Control 3\\n(Brightness)', fontsize=10)\n",
    "axes[2].set_ylim([0, 1])\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "axes[3].plot(time_axis, control_4, 'm-', linewidth=2)\n",
    "axes[3].set_ylabel('Control 4\\n(Noise)', fontsize=10)\n",
    "axes[3].set_ylim([0, 1])\n",
    "axes[3].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Mock EEG Control Signals Over Time', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: These are continuous, smoothly-varying control signals.\")\n",
    "print(\"They are NOT decoded thoughts or mental states - just control parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Audio from Control Signals\n",
    "\n",
    "Now let's hear what these control signals sound like when mapped to synthesis parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset controller and synth\n",
    "controller.reset()\n",
    "synth.reset()\n",
    "\n",
    "# Generate 5 seconds of audio\n",
    "duration = 5.0  # seconds\n",
    "chunk_size = 0.1  # 100ms chunks\n",
    "n_chunks = int(duration / chunk_size)\n",
    "\n",
    "audio_chunks = []\n",
    "control_log = []\n",
    "\n",
    "print(\"Generating audio...\")\n",
    "for i in range(n_chunks):\n",
    "    # Get control vector\n",
    "    controls = controller.get_control_vector(duration=chunk_size)\n",
    "    control_log.append(controls)\n",
    "    \n",
    "    # Generate audio chunk\n",
    "    audio_chunk = synth.generate(chunk_size, controls)\n",
    "    audio_chunks.append(audio_chunk)\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Generated {(i + 1) * chunk_size:.1f}s / {duration}s\")\n",
    "\n",
    "# Concatenate audio\n",
    "audio = np.concatenate(audio_chunks)\n",
    "\n",
    "print(f\"\\n✓ Generated {duration}s of audio ({len(audio)} samples at {synth.sample_rate} Hz)\")\n",
    "print(\"\\nPlay the audio below:\")\n",
    "\n",
    "# Display audio player\n",
    "display(Audio(audio, rate=synth.sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Audio and Control Parameters Together\n",
    "\n",
    "Let's see how the control signals correspond to the audio output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract control parameters\n",
    "control_1 = [c['control_1'] for c in control_log]\n",
    "control_2 = [c['control_2'] for c in control_log]\n",
    "\n",
    "# Downsample audio for visualization\n",
    "downsample_factor = 1000\n",
    "audio_downsampled = audio[::downsample_factor]\n",
    "time_audio = np.arange(len(audio_downsampled)) * downsample_factor / synth.sample_rate\n",
    "\n",
    "# Time axis for controls\n",
    "time_controls = np.arange(len(control_1)) * chunk_size\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "\n",
    "# Audio waveform\n",
    "axes[0].plot(time_audio, audio_downsampled, 'k-', linewidth=0.5, alpha=0.7)\n",
    "axes[0].set_ylabel('Audio\\nAmplitude', fontsize=10)\n",
    "axes[0].set_xlim([0, duration])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_title('Generated Audio Waveform', fontsize=11)\n",
    "\n",
    "# Control 1 (Tempo/Density)\n",
    "axes[1].plot(time_controls, control_1, 'b-', linewidth=2)\n",
    "axes[1].set_ylabel('Control 1\\n(Tempo/Density)', fontsize=10)\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].set_xlim([0, duration])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Control 2 (Harmonic Tension)\n",
    "axes[2].plot(time_controls, control_2, 'g-', linewidth=2)\n",
    "axes[2].set_ylabel('Control 2\\n(Harmony)', fontsize=10)\n",
    "axes[2].set_ylim([0, 1])\n",
    "axes[2].set_xlim([0, duration])\n",
    "axes[2].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axhline(y=0.5, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Audio Output vs Control Parameters', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObserve how changes in control parameters affect the audio:\")\n",
    "print(\"- Higher Control 1 → More frequent events (denser rhythm)\")\n",
    "print(\"- Higher Control 2 → More dissonant harmonics (tension)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Control (Simple Version)\n",
    "\n",
    "Let's try manually setting control values to hear the effect.\n",
    "This demonstrates that **you are in control**, not some autonomous AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate audio with specific control values\n",
    "def generate_with_controls(c1, c2, c3, c4, duration=2.0):\n",
    "    \"\"\"\n",
    "    Generate audio with specific control parameter values.\n",
    "    \n",
    "    Args:\n",
    "        c1: tempo_density (0-1)\n",
    "        c2: harmonic_tension (0-1)\n",
    "        c3: spectral_brightness (0-1)\n",
    "        c4: noise_balance (0-1)\n",
    "        duration: Length in seconds\n",
    "    \"\"\"\n",
    "    controls = {\n",
    "        'control_1': c1,\n",
    "        'control_2': c2,\n",
    "        'control_3': c3,\n",
    "        'control_4': c4,\n",
    "    }\n",
    "    \n",
    "    synth.reset()\n",
    "    \n",
    "    # Generate audio in chunks\n",
    "    chunk_size = 0.1\n",
    "    n_chunks = int(duration / chunk_size)\n",
    "    audio_chunks = []\n",
    "    \n",
    "    for _ in range(n_chunks):\n",
    "        audio_chunk = synth.generate(chunk_size, controls)\n",
    "        audio_chunks.append(audio_chunk)\n",
    "    \n",
    "    audio = np.concatenate(audio_chunks)\n",
    "    return audio\n",
    "\n",
    "print(\"Try different control combinations:\\n\")\n",
    "\n",
    "# Example 1: Sparse, consonant, bright, tonal\n",
    "print(\"1. Sparse, consonant, bright, tonal\")\n",
    "audio1 = generate_with_controls(0.2, 0.2, 0.8, 0.1)\n",
    "display(Audio(audio1, rate=synth.sample_rate))\n",
    "\n",
    "# Example 2: Dense, tense, dark, noisy\n",
    "print(\"\\n2. Dense, tense, dark, noisy\")\n",
    "audio2 = generate_with_controls(0.8, 0.8, 0.3, 0.7)\n",
    "display(Audio(audio2, rate=synth.sample_rate))\n",
    "\n",
    "# Example 3: Medium everything\n",
    "print(\"\\n3. Medium settings (balanced)\")\n",
    "audio3 = generate_with_controls(0.5, 0.5, 0.5, 0.5)\n",
    "display(Audio(audio3, rate=synth.sample_rate))\n",
    "\n",
    "print(\"\\n✓ Notice how you have direct control over the sound.\")\n",
    "print(\"  This is an instrument, not an autonomous system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Metrics\n",
    "\n",
    "Let's measure the system's responsiveness - a key factor for live performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure latency for different stages\n",
    "n_trials = 100\n",
    "latencies = {\n",
    "    'control_generation': [],\n",
    "    'audio_generation': [],\n",
    "    'total': []\n",
    "}\n",
    "\n",
    "print(\"Measuring system latency...\")\n",
    "\n",
    "for i in range(n_trials):\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # Control generation\n",
    "    t0 = time.time()\n",
    "    controls = controller.get_control_vector(duration=0.1)\n",
    "    t1 = time.time()\n",
    "    latencies['control_generation'].append((t1 - t0) * 1000)  # ms\n",
    "    \n",
    "    # Audio generation\n",
    "    t2 = time.time()\n",
    "    audio = synth.generate(0.1, controls)\n",
    "    t3 = time.time()\n",
    "    latencies['audio_generation'].append((t3 - t2) * 1000)  # ms\n",
    "    \n",
    "    t_end = time.time()\n",
    "    latencies['total'].append((t_end - t_start) * 1000)  # ms\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LATENCY MEASUREMENTS (100 trials)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for stage, times in latencies.items():\n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    min_time = np.min(times)\n",
    "    max_time = np.max(times)\n",
    "    \n",
    "    print(f\"\\n{stage.replace('_', ' ').title()}:\")\n",
    "    print(f\"  Mean: {mean_time:.2f} ms (± {std_time:.2f})\")\n",
    "    print(f\"  Range: {min_time:.2f} - {max_time:.2f} ms\")\n",
    "\n",
    "total_mean = np.mean(latencies['total'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if total_mean < 100:\n",
    "    print(f\"✓ PASS: Total latency ({total_mean:.1f} ms) < 100 ms target\")\n",
    "    print(\"  System is suitable for real-time performance!\")\n",
    "else:\n",
    "    print(f\"⚠ WARNING: Total latency ({total_mean:.1f} ms) > 100 ms target\")\n",
    "    print(\"  May need optimization for real-time use.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot latency distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "axes[0].hist(latencies['control_generation'], bins=20, color='blue', alpha=0.7)\n",
    "axes[0].set_xlabel('Latency (ms)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Control Generation')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(latencies['audio_generation'], bins=20, color='green', alpha=0.7)\n",
    "axes[1].set_xlabel('Latency (ms)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Audio Generation')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].hist(latencies['total'], bins=20, color='red', alpha=0.7)\n",
    "axes[2].axvline(x=100, color='black', linestyle='--', linewidth=2, label='100ms target')\n",
    "axes[2].set_xlabel('Latency (ms)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Total End-to-End')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Latency Distribution (100 trials)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✓ **Real-time control** — Generate control signals from mock EEG\n",
    "2. ✓ **Audio synthesis** — Map controls to sound parameters\n",
    "3. ✓ **Visualization** — See control-sound relationships\n",
    "4. ✓ **Manual control** — Direct parameter manipulation\n",
    "5. ✓ **Performance metrics** — Measure system latency\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **This is a performance instrument**, not brain decoding\n",
    "- **You are in control** — parameters respond to your input\n",
    "- **Low latency** — System is responsive enough for live use\n",
    "- **Transparent** — All mappings are visible and adjustable\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try the **AI co-performer demo** (`ai_co_performer_demo.ipynb`)\n",
    "- Experiment with **custom mappings** (LinearMapper, MLPMapper)\n",
    "- Connect **real EEG/fNIRS** hardware (if available)\n",
    "- Use **keyboard control** for comparison with EEG\n",
    "- **Perform live** with the system!\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: BrainJam is about **performer agency and creative expression**,  \n",
    "not signal accuracy or autonomous AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
