{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Mapping: Brain Features → Music\n",
    "\n",
    "This notebook demonstrates strategies for mapping neural features to music model latent spaces.\n",
    "\n",
    "## Approaches\n",
    "1. **Linear Mapping** - Simple projections from brain to latent space\n",
    "2. **Neural Network Mapping** - Learn nonlinear transformations\n",
    "3. **Semantic Conditioning** - Use text/CLAP embeddings as intermediary\n",
    "4. **Direct Parameter Control** - Map to interpretable synthesis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulate Data\n",
    "\n",
    "Create synthetic brain features and music latent vectors for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate brain features (like EEG or fMRI features)\n",
    "n_samples = 200\n",
    "n_brain_features = 10  # e.g., band powers, connectivity, etc.\n",
    "n_latent_dims = 8  # Target music latent space dimensions\n",
    "\n",
    "# Generate synthetic brain features\n",
    "brain_features = np.random.randn(n_samples, n_brain_features)\n",
    "\n",
    "# Generate synthetic music latent vectors\n",
    "# In reality, these would come from a pretrained music model (MusicVAE, etc.)\n",
    "music_latents = np.random.randn(n_samples, n_latent_dims)\n",
    "\n",
    "# Create a synthetic relationship (for demonstration)\n",
    "# Real data would come from paired brain-music recordings\n",
    "true_mapping = np.random.randn(n_brain_features, n_latent_dims) * 0.5\n",
    "music_latents = brain_features @ true_mapping + 0.3 * np.random.randn(n_samples, n_latent_dims)\n",
    "\n",
    "print(f\"Brain features shape: {brain_features.shape}\")\n",
    "print(f\"Music latents shape: {music_latents.shape}\")\n",
    "\n",
    "# Split into train/test\n",
    "split_idx = int(0.8 * n_samples)\n",
    "X_train, X_test = brain_features[:split_idx], brain_features[split_idx:]\n",
    "y_train, y_test = music_latents[:split_idx], music_latents[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Mapping\n",
    "\n",
    "Simple ridge regression to map brain features to music latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "# Train ridge regression\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict\n",
    "y_pred_scaled = ridge_model.predict(X_test_scaled)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Evaluate\n",
    "mse = np.mean((y_test - y_pred) ** 2)\n",
    "r2 = ridge_model.score(X_test_scaled, y_test_scaled)\n",
    "\n",
    "print(f\"Linear Mapping Performance:\")\n",
    "print(f\"  MSE: {mse:.4f}\")\n",
    "print(f\"  R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for dim in range(n_latent_dims):\n",
    "    axes[dim].scatter(y_test[:, dim], y_pred[:, dim], alpha=0.6)\n",
    "    axes[dim].plot([y_test[:, dim].min(), y_test[:, dim].max()],\n",
    "                   [y_test[:, dim].min(), y_test[:, dim].max()],\n",
    "                   'r--', linewidth=2)\n",
    "    axes[dim].set_xlabel('True Latent')\n",
    "    axes[dim].set_ylabel('Predicted Latent')\n",
    "    axes[dim].set_title(f'Dimension {dim+1}')\n",
    "    axes[dim].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Linear Mapping: True vs Predicted Latent Dimensions', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network Mapping\n",
    "\n",
    "Learn a nonlinear mapping using a small neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainToMusicMapper(nn.Module):\n",
    "    \"\"\"Neural network for brain → music latent mapping\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create model\n",
    "model = BrainToMusicMapper(\n",
    "    input_dim=n_brain_features,\n",
    "    hidden_dim=32,\n",
    "    output_dim=n_latent_dims\n",
    ")\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.FloatTensor(X_train_scaled)\n",
    "y_train_t = torch.FloatTensor(y_train_scaled)\n",
    "X_test_t = torch.FloatTensor(X_test_scaled)\n",
    "y_test_t = torch.FloatTensor(y_test_scaled)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "n_epochs = 200\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred_t = model(X_train_t)\n",
    "    loss = criterion(y_pred_t, y_train_t)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n✓ Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate neural network\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_nn_t = model(X_test_t)\n",
    "    test_loss = criterion(y_pred_nn_t, y_test_t).item()\n",
    "    \n",
    "y_pred_nn = scaler_y.inverse_transform(y_pred_nn_t.numpy())\n",
    "\n",
    "mse_nn = np.mean((y_test - y_pred_nn) ** 2)\n",
    "\n",
    "print(f\"Neural Network Mapping Performance:\")\n",
    "print(f\"  MSE: {mse_nn:.4f}\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison: Linear vs Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods\n",
    "comparison = {\n",
    "    'Method': ['Linear (Ridge)', 'Neural Network'],\n",
    "    'MSE': [mse, mse_nn],\n",
    "    'Complexity': ['Low', 'Medium'],\n",
    "    'Real-time': ['✓ Fast', '✓ Fast'],\n",
    "    'Interpretability': ['High', 'Low']\n",
    "}\n",
    "\n",
    "print(\"\\nMethod Comparison:\")\n",
    "print(\"=\"*60)\n",
    "for key in comparison.keys():\n",
    "    print(f\"{key:20s} {str(comparison[key][0]):20s} {str(comparison[key][1]):20s}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Conditioning Strategy\n",
    "\n",
    "Alternative: Map brain states to semantic descriptions, then use text-to-music models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define semantic music dimensions\n",
    "semantic_dims = [\n",
    "    ('energy', ['calm', 'moderate', 'energetic']),\n",
    "    ('valence', ['sad', 'neutral', 'happy']),\n",
    "    ('complexity', ['simple', 'moderate', 'complex']),\n",
    "    ('tempo', ['slow', 'medium', 'fast'])\n",
    "]\n",
    "\n",
    "print(\"Semantic Conditioning Approach:\")\n",
    "print(\"\\nBrain Features → Semantic Attributes → Text Description → Music\")\n",
    "print(\"\\nExample mappings:\")\n",
    "print(\"  High engagement + positive asymmetry → 'energetic happy music'\")\n",
    "print(\"  High theta/alpha + low engagement → 'calm contemplative ambient'\")\n",
    "print(\"  High beta + negative asymmetry → 'tense focused electronic'\")\n",
    "\n",
    "# Example: Construct text description from features\n",
    "def brain_to_text_description(brain_features):\n",
    "    \"\"\"Convert brain features to music description\"\"\"\n",
    "    # Simplified example\n",
    "    engagement = brain_features[0]\n",
    "    valence = brain_features[1]\n",
    "    complexity = brain_features[2]\n",
    "    \n",
    "    energy = 'energetic' if engagement > 0.5 else 'calm'\n",
    "    mood = 'happy' if valence > 0 else 'melancholic'\n",
    "    texture = 'complex' if complexity > 0.5 else 'simple'\n",
    "    \n",
    "    return f\"{energy} {mood} music with {texture} harmonies\"\n",
    "\n",
    "# Example\n",
    "example_features = np.array([0.8, 0.3, -0.2])\n",
    "description = brain_to_text_description(example_features)\n",
    "print(f\"\\nExample: Features {example_features} → '{description}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Direct Parameter Control (DDSP-style)\n",
    "\n",
    "Map brain features to interpretable synthesis parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define interpretable audio parameters\n",
    "audio_params = {\n",
    "    'fundamental_frequency': (100, 500),  # Hz\n",
    "    'loudness': (0.1, 1.0),  # Amplitude\n",
    "    'harmonic_distribution': (0, 1),  # Brightness\n",
    "    'noise_amount': (0, 0.5),  # Texture\n",
    "    'reverb': (0, 1),  # Spaciousness\n",
    "    'attack_time': (0.01, 0.5)  # Seconds\n",
    "}\n",
    "\n",
    "print(\"Direct Parameter Mapping:\")\n",
    "print(\"\\nBrain Feature → Audio Parameter:\")\n",
    "print(\"  Engagement → Tempo/Energy\")\n",
    "print(\"  Frontal Asymmetry → Harmonic Mode (major/minor)\")\n",
    "print(\"  Theta/Alpha Ratio → Harmonic Complexity\")\n",
    "print(\"  Overall Activation → Loudness\")\n",
    "print(\"  DMN Connectivity → Reverb/Spaciousness\")\n",
    "\n",
    "def map_brain_to_audio_params(brain_features):\n",
    "    \"\"\"Map brain features to audio synthesis parameters\"\"\"\n",
    "    params = {}\n",
    "    \n",
    "    # Example mappings (would be learned or designed)\n",
    "    params['fundamental_frequency'] = 200 + 200 * brain_features[0]  # Engagement → pitch\n",
    "    params['loudness'] = 0.3 + 0.5 * brain_features[1]  # Activation → volume\n",
    "    params['harmonic_distribution'] = (brain_features[2] + 1) / 2  # Normalize to [0, 1]\n",
    "    params['noise_amount'] = max(0, brain_features[3] * 0.3)\n",
    "    params['reverb'] = (brain_features[4] + 1) / 2\n",
    "    params['attack_time'] = 0.01 + 0.2 * (1 - brain_features[0])  # Fast attack when engaged\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Example\n",
    "example_brain = np.random.randn(5)\n",
    "audio_params_out = map_brain_to_audio_params(example_brain)\n",
    "\n",
    "print(\"\\nExample audio parameters:\")\n",
    "for param, value in audio_params_out.items():\n",
    "    print(f\"  {param}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "### Mapping Strategies Compared\n",
    "\n",
    "| Strategy | Pros | Cons | Best For |\n",
    "|----------|------|------|----------|\n",
    "| **Linear Mapping** | Simple, interpretable, fast | Limited expressiveness | Initial prototyping |\n",
    "| **Neural Network** | Captures nonlinear relationships | Needs training data, less interpretable | Performance optimization |\n",
    "| **Semantic Conditioning** | Leverages pretrained models, intuitive | Indirect control | Text-to-music models |\n",
    "| **Direct Parameters** | Highly interpretable, real-time | Requires parameter design | DDSP-like synthesis |\n",
    "\n",
    "### Recommended Approach\n",
    "\n",
    "1. **Start with direct parameter mapping**\n",
    "   - Design interpretable mappings\n",
    "   - Test with users for intuitive feel\n",
    "   - Iterate based on feedback\n",
    "\n",
    "2. **Add semantic layer**\n",
    "   - Map brain states to high-level descriptors\n",
    "   - Use for text-to-music conditioning\n",
    "   - Provides user-understandable explanations\n",
    "\n",
    "3. **Optimize with learning**\n",
    "   - Collect paired brain-music preferences\n",
    "   - Train neural network for personalization\n",
    "   - Keep interpretable fallback\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- **Agency**: Users should understand and control the mapping\n",
    "- **Latency**: Real-time requires <100ms end-to-end\n",
    "- **Personalization**: Individual differences necessitate calibration\n",
    "- **Stability**: Mappings should be smooth and predictable\n",
    "- **Expressiveness**: Balance control and autonomy\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Test mappings with real neural data\n",
    "2. Implement in real-time system (see `toy_interface/`)\n",
    "3. Conduct user studies for validation\n",
    "4. Iterate based on user experience"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
